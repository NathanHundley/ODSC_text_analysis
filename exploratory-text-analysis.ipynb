{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Text Analysis in Python using spaCy and textacy\n",
    "\n",
    "Workshop for Open Data Science Conference East 2021\n",
    "\n",
    "[Workshop materials](https://github.com/csbailey5t/ODSC_text_analysis)\n",
    "\n",
    "Scott Bailey <br/>\n",
    "Digital Research and Scholarship Librarian <br/>\n",
    "Copyright and Digital Scholarship Center <br/>\n",
    "NC State University Libraries\n",
    "\n",
    "## Outline\n",
    "1. Intro and overview of NLP libraries\n",
    "2. Document-level analysis <br/>\n",
    "    a. Tokenization <br/>\n",
    "    b. Cleaning text data <br />\n",
    "    c. Part-of-speech tagging <br/>\n",
    "    d. Named entity recognition <br/>\n",
    "    e. Similarity vectors <br/>\n",
    "    f. Rule-based matching <br />\n",
    "3. Scaling up to corpus-level analysis\n",
    "4. Further resources for spaCy\n",
    "\n",
    "## Learning goal:\n",
    "\n",
    "Through the course of the workshop, you'll practice using the core NLP features of spaCy and textacy, and connect those features to exploratory questions. \n",
    "\n",
    "## What do we mean by \"exploratory text analysis?\"\n",
    "- How clean are the data?\n",
    "- What methods do the data support?\n",
    "- Project scoping \n",
    "- Research question refinement\n",
    "- Iterative research \n",
    "\n",
    "## A quick(!) overview of NLP-related libraries in Python\n",
    "- [nltk](https://www.nltk.org/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/)\n",
    "- [stanza/corenlp](https://stanfordnlp.github.io/stanza/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [huggingface transformers - pytorch and tensorflow](https://github.com/huggingface/transformers)\n",
    "\n",
    "### Why spaCy and textacy?\n",
    "\n",
    "SpaCy is an opinionated, performant NLP library that does a lot of the work for you while revealing where you might need to do more custom refinement or model building. Textacy builds smoothly on spaCy to add corpus analysis and common information retrieval methods.\n",
    "\n",
    "## Questions during the workshop\n",
    "\n",
    "During the workshop, please do ask questions by way of the chat. I'll be keeping an eye on that, and will answer questions as we go if I can. I'll also give some time during and after the workshop when folks can unmute and ask questions. \n",
    "\n",
    "## Jupyter Notebooks, Colab, and Binder\n",
    "\n",
    "If you haven't worked with [Jupyter](https://jupyter.org/) notebooks before, they are a widely-used literate programming tool that let you write and execute cells of code. \n",
    "\n",
    "[Google Colab](https://colab.research.google.com) is a hosted notebook environment from Google, which provides free access to limited GPU resources.\n",
    "\n",
    "[Binder](https://mybinder.org/) is a great project that builds reproducible environments to execute Jupyter notebooks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if working in Colab\n",
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally, you can also run this in your terminal with an active virtual environment\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://en.wikipedia.org/wiki/Data_science\n",
    "sample_text = \"\"\"Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains. Data science is related to data mining, machine learning and big data.\n",
    "\n",
    "Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" in order to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. Turing award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in doc[:20]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One of the common things we do in text analysis is to remove punctuation\n",
    "no_punct = [token for token in doc if token.is_punct == False]\n",
    "for token in no_punct[50:100]:\n",
    "  print(token.text, token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This has worked, but left in new line characters and spaces\n",
    "no_punct_or_space = [token for token in doc if token.is_punct == False and token.is_space == False]\n",
    "for token in no_punct_or_space[50:100]:\n",
    "  print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we also want to remove numbers, and lowercase everything\n",
    "lower_alpha = [token.lower_ for token in no_punct_or_space if token.is_alpha == True]\n",
    "lower_alpha[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other common bit of preprocessing is to remove stopwords, that is, the common words in a language that don't convey the information that we are looking for in our analysis. For example, if we looked for the most common words in a text, we would want to remove stopwords so that we don't only get words such as 'a,' 'the,' and 'and.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = [token.lower_ for token in no_punct_or_space if token.is_alpha == True and token.is_stop == False]\n",
    "clean[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this piece, we've used spaCy's built in stopword list, which is used to create the property `is_stop` for each token. There's a good chance you would want to create custom stopwords lists though, especially if you're working with historical text or really domain-specific text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just pick a couple of words we know are in the example\n",
    "custom_stopwords = [\"data\", \"algorithms\"]\n",
    "\n",
    "custom_clean = [token.lower_ for token in doc if token.lower_ not in custom_stopwords]\n",
    "custom_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a list of lower-cased tokens that doesn't contain punctuation, white-space, numbers, or stopwords. Depending on our analysis, we may or may not want to do this much cleaning. But, it is good to understand how much we can do just with spaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we can break apart the document and filter it now, it's a good time to start counting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of tokens in document: \", len(doc))\n",
    "print(\"Number of tokens in cleaned document: \", len(clean))\n",
    "print(\"Number of unique tokens in cleaned document: \", len(set(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sentences\n",
    "len(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all lower-cased tokens\n",
    "full_counter = Counter([token.lower_ for token in doc])\n",
    "full_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count cleaned tokens\n",
    "cleaned_counter = Counter(clean)\n",
    "cleaned_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "In the cell below, write code to find the five most common noun chunks in the original doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coarse grained UPOS: https://universaldependencies.org/docs/u/pos/\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-grained POS, Penn Treebank: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "for token in doc[:20]:\n",
    "    print(token.text, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure what those tags are? Try spaCy's explain function\n",
    "spacy.explain(\"DT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tokens by part of speech\n",
    "verbs = [token for token in doc if token.pos_ == \"VERB\"]\n",
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect plural nouns\n",
    "nouns_pl = [token for token in doc if token.tag_ == \"NNS\" or token.tag_ == \"NNPS\"]\n",
    "nouns_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency tree visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentence = list(doc.sents)[0]\n",
    "single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spaCy determines the dependency tree for it's doc. Like POS, we can see the dependency tags of each token. \n",
    "for token in single_sentence:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(single_sentence, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "\n",
    "[List of entity types in this spaCy model](https://spacy.io/models/en#en_core_web_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "Add or modify a sentence in the original `sample_text` so that spaCy will detect a GPE. Then, in the cell below, write code to return a list of all entities that are either PERSON or GPE.\n",
    "\n",
    "**hint**: make sure to reprocess the `sample_text` with the `nlp` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentence = list(doc.sents)[-1]\n",
    "displacy.render(single_sentence, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word, sentence, and document vectors\n",
    "\n",
    "SpaCy's medium (`md`) and large (`lg`) models include GloVe word vectors trained on the [Common Crawl](https://commoncrawl.org/). \n",
    "\n",
    "You could train your own vectors with `gensim` and `word2vec`, use a large language model, or many other libraries and algorithms. But, if you're text is fairly recent and especially from the web, the common crawl vectors might be enough, especially for exploratory work. \n",
    "\n",
    "`Token`s have vectors. `Doc`s and `Span`s have vectors that are the average of their token vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token vectors\n",
    "for token in doc[:5]:\n",
    "    print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc vector\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence/span vector\n",
    "list(doc.sents)[0].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fine, but for exploratory work, we might just be interested in some similarity measures between tokens, sentences, or documents. SpaCy uses the common cosine similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token1 in doc[:10]:\n",
    "    for token2 in doc[:10]:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Looking at the results, can you explain the scale of the similarity score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent1 in doc.sents:\n",
    "    for sent2 in doc.sents:\n",
    "        print(sent1.text, \"\\n\", sent2.text, \"\\n\", sent1.similarity(sent2))\n",
    "        print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule based matcher\n",
    "\n",
    "Rule-based matching is an incredibly powerful complement to the statistic models of spaCy. It's also a bit complex though, and it's worth looking at the docs [here](https://spacy.io/usage/rule-based-matching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Available token attributes for the `Matcher` pattern](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll define a pattern as a list of dictionaries, where each dictionary describes a token\n",
    "pattern = [{'LOWER': 'data'},\n",
    "           {'POS': 'NOUN'}]\n",
    "# The Matcher expects a list of patterns\n",
    "matcher.add(\"data+noun\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the easiest ways to build up these `Matcher` patterns is to use Explosion's online [Rule-based Matcher Explorer](https://explosion.ai/demos/matcher). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with multiple documents (a corpus)\n",
    "\n",
    "For a small corpus, you can build a list or dictionary of processed spaCy docs. Once you have that list or dictionary, approach it in terms of using the type of code we've written above, but applied over the larger data structure. \n",
    "\n",
    "For larger corpora, though, you might need to think about streaming data or distributed processing. \n",
    "\n",
    "We're going to turn to textacy to work with a corpus of documents shortly, but it is useful to think about how you can use standard data structures in combination with spaCy as needed, and when it is enough for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've cloned the repo, or are working in Binder, the data are there in the `sotu` directory. If in Colab, you'll need to run the cell below to download the zip archive, then unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if within Colab to download and unzip data\n",
    "!wget https://github.com/csbailey5t/ODSC_text_analysis/raw/master/archive.zip\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(\"/content/archive.zip\", 'r') as zobj:\n",
    "  zobj.extractall(path=\"sotu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the paths for all .txt files in our data directory\n",
    "fns = glob.glob(\"sotu/*.txt\")\n",
    "len(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for fn in fns:\n",
    "    with open(fn, 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time corpus = [nlp(text) for text in texts[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for doc in corpus[:2]:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all geo-political entities from whole corpus\n",
    "gpes = [(ent.text, ent.label_) for ent in doc.ents for doc in corpus if ent.label_ == \"GPE\"]\n",
    "len(gpes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpes[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the set of unique GPEs\n",
    "set(gpes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "Choose a method from the single document analysis portion of the workshop, and apply it to this small corpus. For example, you could find the most common words, create a cleaned corpus, or aggregate parts of speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy also provides a `pipe` method on the language model that will process texts in a stream. This can be useful for larger collections of texts, especially when combined with disabling parts of the pipeline you aren't using. \n",
    "\n",
    "https://spacy.io/api/language#pipe\n",
    "\n",
    "Below are timed examples for building the corpus with a standard list comprehension vs the `pipe` method with batching and multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time docs = [nlp(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time docs = list(nlp.pipe(texts, batch_size=10, n_process=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how to build a corpus with textacy now.\n",
    "\n",
    "Textacy corpora can be built directly from a list of texts, or could be buit from texts plus metadata, allowing you to filter the corpus on metadata. For now, we'll stick with just the texts. \n",
    "\n",
    "The full docs for textacy are [here](https://textacy.readthedocs.io/en/stable/), with details on the `Corpus` class [here](https://textacy.readthedocs.io/en/stable/api_reference/lang_doc_corpus.html#module-textacy.corpus). The `Corpus` class does provide convenience functions for saving and loading processed corpora. \n",
    "\n",
    "Before we run the next cells, if you're on Binder, you'll need to do one thing to deal with Binder's memory limitations. In the \"Kernel\" menu, hit \"Restart\". You'll then need to rerun the first four code cells of the notebook, to reimport libraries and initialize the nlp model. After that, skip back down to this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Binder, you'll need to rerun this line, but not in Colab\n",
    "fns = glob.glob(\"sotu/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Colab, we'll stick with 20 texts\n",
    "# In Binder, I recommend dropping to 5\n",
    "texts = []\n",
    "for fn in fns[:20]:\n",
    "    with open(fn, 'r') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = textacy.Corpus(nlp, data=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of documents: \", corpus.n_docs)\n",
    "print(\"number of sentences: \", corpus.n_sents)\n",
    "print(\"number of tokens: \", corpus.n_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pass as_strings so that the results we look at will give us strings rather than unique ids.\n",
    "counts = corpus.word_counts(as_strings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, by default, the word_counts function is doing a certain amount of cleaning for you: https://chartbeat-labs.github.io/textacy/api_reference/lang_doc_corpus.html#textacy.corpus.Corpus.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an explanation of -PRON-, see https://spacy.io/api/annotation#lemmatization. Basically it's spaCy's way of lemmatizing pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_counts = corpus.word_doc_counts(weighting=\"freq\", smooth_idf=True, filter_stops=True, as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_doc_counts.items(), key=lambda x:x[1], reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that these are not tf-idf values, which are term frequencies for individual docs weighted by the inverse document frequency. This is a measure of the number of docs the words appear in weighted by inverse document frequency. We're still getting a sense of which words across the corpus and in the context of the corpus seem to have the most importance, if document frequency is a proxy for importance.\n",
    "\n",
    "Textacy provides access to different algorithms that can be run on docs, such as TextRank for keyword extraction. We'll start by working on a single doc, and then look at how we might scale up to thinking about the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.ke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_terms_textrank = textacy.ke.textrank(corpus[4])\n",
    "key_terms_textrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we'll take a look at another algorithm, Yake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_terms_yake = textacy.ke.yake(corpus[4])\n",
    "key_terms_yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's think about aggregating keywords over part of the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_terms_yake_corpus = [textacy.ke.yake(doc) for doc in corpus[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_terms_yake_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_terms_tuples = list(chain(*key_terms_yake_corpus))\n",
    "flat_terms_tuples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have a flat list of tuples, but let's shift to a flat list of just the keys in order to \n",
    "# count the most common keys\n",
    "flat_terms = [k for k,v in flat_terms_tuples]\n",
    "flat_terms[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_counter = Counter(flat_terms)\n",
    "keyword_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword in Context\n",
    "\n",
    "One thing that researchers often find helpful in working with text is simply seeing keywords in context. Maybe you already know terms of interest in your data, but if not, the keyword extract above might help surface interesting words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic_gens = [textacy.text_utils.KWIC(doc.text, \"Nation\") for doc in corpus[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kwic_gen in kwic_gens:\n",
    "  for entry in kwic_gen:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textacy includes a lot of great information extraction and analysis features, including built-in [corpus vectorization](https://textacy.readthedocs.io/en/stable/api_reference/vsm_and_tm.html#module-textacy.vsm.vectorizers) and [topic modeling](https://textacy.readthedocs.io/en/stable/api_reference/vsm_and_tm.html#textacy.tm.topic_model.TopicModel) by way of [scikit-learn](https://scikit-learn.org/stable/). It also has [text pre-processing](https://textacy.readthedocs.io/en/stable/api_reference/text_processing.html) utilities with sensible defaults. \n",
    "\n",
    "In [information extraction](https://textacy.readthedocs.io/en/stable/api_reference/information_extraction.html) there are great tools to extract common structures, such as subject-verb-object triples and direct quotations.\n",
    "\n",
    "While the current version of textacy doesn't support spaCy v3, the main developer, Burton DeWilde, is actively working on updating textacy for compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources for spaCy\n",
    "\n",
    "- [spaCy 101](https://spacy.io/usage/spacy-101) - spaCy's own intro documentation\n",
    "- [Advanced NLP with spaCy](https://course.spacy.io/) - spaCy's own interactive learning course; you don't need to be \"ready\" for \"advanced\" work to benefit from going through this course\n",
    "- [textacy](https://github.com/chartbeat-labs/textacy) - a Python library built on top of spaCy and scikit-learn to faciliate working with a corpus and providing extra functionality\n",
    "- [spaCy universe](https://spacy.io/universe) - extensive collection of packages built on top of or with spaCy for various NLP and text analysis tasks\n",
    "- [spaCy youtube videos](https://www.youtube.com/c/ExplosionAI/videos) - Explosion has a lot of great videos on Youtube, and there are a number of other folks who have created great walkthroughs of using different parts of spaCy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('3.8.0': pyenv)",
   "name": "python380jvsc74a57bd0c7fa88980dda015734655e9ba7924d2d70ca0094b443be466d5e68bf327a7fc4"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}